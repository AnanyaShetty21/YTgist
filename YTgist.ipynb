{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnanyaShetty21/YTgist/blob/main/YTgist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "video_id = \"6mgkoqcm6Sg\"\n",
        "api_key = getpass.getpass(\"Enter the API key: \")"
      ],
      "metadata": {
        "id": "tEhd-nssv0fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports"
      ],
      "metadata": {
        "id": "5SEPwAGCwOmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube_transcript_api\n",
        "!pip install pytextrank\n",
        "!pip install plotly nltk\n",
        "!pip install wordcloud matplotlib\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "JJOG_ODbwNl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import plotly.express as px\n",
        "import nltk\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re"
      ],
      "metadata": {
        "id": "0UpgYNifweKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU"
      ],
      "metadata": {
        "id": "bocUheLLwWGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ou2Gvbm6wXTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the transcript through youtube-transcript-api"
      ],
      "metadata": {
        "id": "7EXeameOvnHJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB4gTk19vgTr"
      },
      "outputs": [],
      "source": [
        "ytt_api = YouTubeTranscriptApi()\n",
        "transcript = ytt_api.fetch(video_id).to_raw_data()\n",
        "text = ' '.join([entry['text'] for entry in transcript])\n",
        "text = text.replace('\\n',' ')\n",
        "text = text.replace(\"\\\\\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarizing using pre-trained model"
      ],
      "metadata": {
        "id": "wEz4xNUxw90r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/led-large-16384-pubmed\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"patrickvonplaten/led-large-16384-pubmed\").to(device)"
      ],
      "metadata": {
        "id": "vE5f1q_lvzFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(text, truncation = False, padding = \"max_length\", return_tensors = \"pt\")\n",
        "tokens = {key: value.to(device) for key, value in tokens.items()}\n",
        "summary = model.generate(**tokens)\n",
        "summary_decoded = tokenizer.decode(summary[0])\n",
        "print(summary_decoded)"
      ],
      "metadata": {
        "id": "PpMc_rAxxKsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract key words using textrank"
      ],
      "metadata": {
        "id": "RfvWszBpnOJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pytextrank\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"textrank\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "top_phrases = [phrase.text for phrase in doc._.phrases[:10]]\n",
        "\n",
        "for phrase in top_phrases:\n",
        "    print(phrase)\n"
      ],
      "metadata": {
        "id": "3dhvvxkbkj37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a timeline of key moments in the video"
      ],
      "metadata": {
        "id": "cdMFWNT2nUBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting key moments from the video description\n",
        "from googleapiclient.discovery import build\n",
        "from IPython.display import JSON\n",
        "\n",
        "def get_video_description(video_id):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet\",\n",
        "        id=video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    if \"items\" in response and len(response[\"items\"]) > 0:\n",
        "        description = response[\"items\"][0][\"snippet\"][\"description\"]\n",
        "        return description\n",
        "\n",
        "description = get_video_description(video_id)\n",
        "\n",
        "def get_lines_with_timestamps(description):\n",
        "    unique_pairs = set()\n",
        "    lines = description.split(\"\\n\")\n",
        "    for i in range(len(lines)):\n",
        "        pairs = []\n",
        "        for line in lines:\n",
        "            line_with_timestamps = re.findall(r'\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b', line)\n",
        "            if line_with_timestamps:\n",
        "                for timestamp in line_with_timestamps:\n",
        "                    pairs.append((timestamp, line.replace(timestamp, \"\").strip()))\n",
        "                    unique_pairs.update(pairs)\n",
        "\n",
        "    return sorted(unique_pairs, key=lambda x: x[0])\n",
        "\n",
        "key_moments_description = get_lines_with_timestamps(description)"
      ],
      "metadata": {
        "id": "Y544w_N92ryj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting key moments from the transcript\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = []\n",
        "for line in transcript:\n",
        "    corpus.append(line['text'])\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "\n",
        "importance = X.sum(axis=1).A1\n",
        "important_indices = importance.argsort()[-5:][::-1]\n",
        "\n",
        "key_moments = [(transcript[i]['start'], transcript[i]['text']) for i in important_indices]\n"
      ],
      "metadata": {
        "id": "vBsWhqj5nQsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.DataFrame(key_moments, columns=[\"Timestamp\", \"Key Moment\"])\n",
        "df_description = pd.DataFrame(key_moments_description, columns=[\"Timestamp\", \"Key Moment\"])\n",
        "\n",
        "df_description[\"Timestamp\"] = df_description[\"Timestamp\"].astype(str)\n",
        "\n",
        "for i in range(len(df_description[\"Timestamp\"])):\n",
        "    time = df_description.loc[i,\"Timestamp\"].split(\":\")\n",
        "    if(len(time)==1):\n",
        "      seconds = int(time[0])\n",
        "    elif(len(time)==2):\n",
        "      seconds = int(time[0]) * 60 + int(time[1])\n",
        "    elif(len(time)==3):\n",
        "      seconds = int(time[0]) * 3600 + int(time[1]) * 60 + int(time[2])\n",
        "    df_description.loc[i, \"Timestamp\"] = seconds\n",
        "\n",
        "\n",
        "\n",
        "fig = px.scatter(df, x=\"Timestamp\", y=\"Key Moment\", text=\"Key Moment\", title=\"Timeline of Key Moments in Video\")\n",
        "\n",
        "fig.add_scatter(x = df_description[\"Timestamp\"], y = df_description[\"Key Moment\"], mode=\"markers+text\", marker=dict(color=\"red\", size=10), name=\"Description Moments\", text=df_description[\"Key Moment\"], textposition=\"top center\")\n",
        "\n",
        "fig.update_traces(textposition=\"top center\")\n",
        "fig.update_layout(xaxis_title=\"Timestamp in milliseconds\")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "DZn9sAK2ktDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wordcloud representation of the video"
      ],
      "metadata": {
        "id": "wDflHyDInZ_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = \"\"\n",
        "for line in transcript:\n",
        "    corpus = corpus + line['text']\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=50)\n",
        "X = vectorizer.fit_transform([corpus])\n",
        "important_words = dict(zip(vectorizer.get_feature_names_out(), X.toarray()[0]))\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(important_words)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LlPZYF-d1kOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpLiVi9FfDTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}